## Esercizio A.4.1 NQueen

NQueen è stato modellando usando l'interfaccia ```Problem```, essa permette
di modellare un problema e di poterlo risolvere decidendo un algorimo.

I test sono stati fatti su un istanza del problema 8x8.

```rust
    fn heuristic(&self, state: &Self::State) -> Self::Cost {
        let mut result = 0.0;

        for i in 0..self.n {
            for j in (i + 1)..self.n {
                if state.pos[i] == state.pos[j] {
                    result += 1.0;
                    // break;
                }
            }

            for j in (i + 1)..self.n {
                if state.pos[i].abs_diff(state.pos[j]) == i.abs_diff(j) {
                    result += 1.0;
                    // break;
                }
            }
        }

        return result.into();
    }
```

Il problema è stato implementato come è stato spiegato a lezione, l'unica differenza che ho attuato per migliorare
la correctness è stata di non considerare il numero di regine in attacco, ma anche il numero di quelle che stanno
sulla stessa riga e diagonale.

Per esempio consideriamo queste due configurazioni.
```
    Q . Q Q
    . . . .
    . Q . .

    Q . Q .
    . . . .
    . Q . Q
```
Se consideriamo semplicemente il numero di regine in attacco sono in entrambi i casi 3 coppie di regine.
Invece se commentiamo i ```break``` puniamo l'algoritmo se mette piu regine sulla stessa riga o colonna.

# Steepest Descend

Implementazione Rust dell'algoritmo

```rust
fn attempt(&mut self, problem: &P) -> AttemptResult<P> {
        let mut iterations = 0;
        let mut curr_state = problem.random_state(&mut self.rng);
        let mut curr_h = problem.heuristic(&curr_state);
        loop {
            iterations += 1;
            let mut new_curr_state = curr_state.clone();
            let mut new_curr_h = curr_h;
            for a in problem.executable_actions(&curr_state) {
                let (new_state, _) = problem.result(&curr_state, &a);
                let new_h = problem.heuristic(&new_state);
                if new_h < new_curr_h {
                    new_curr_state = new_state;
                    new_curr_h = new_h;
                }
            }
            if curr_h > new_curr_h {
                curr_state = new_curr_state;
                curr_h = new_curr_h;
            } else {
                let result = AttemptResult::new(curr_state, curr_h, iterations);
                return result;
            }
        }
    }
```

Questa è solo la formulazione di "un testativo" dell'algoritmo per trovare l'ottimo.

Il Risolutore ```Resolver``` ha in piu' un metodo per implementa la possibilità di rieseguire l'algoritmo
e prendere il risultato ottimo.

```rust
    pub fn resolve_restart(&mut self, problem: &P, max_restarts: usize) -> ResolverResult<P> {
        let start = Instant::now();
        let mut result = self.algo.attempt(problem);
        for _ in 1..max_restarts {
            let new_result = self.algo.attempt(problem);
            if new_result.h <= P::Cost::default() {
                // TODO: check if it is a goal state
                result.state = new_result.state;
                result.h = new_result.h;
                result.iterations += new_result.iterations;
                let result = ResolverResult::from_inner(start, result);
                return result;
            }
            if new_result.h < result.h {
                result.state = new_result.state;
                result.h = new_result.h;
            }
            result.iterations += new_result.iterations;
        }
        return ResolverResult::from_inner(start, result);
    }
```

Da notare che questo codice è generico rispetto all'algoritmo utilizzato. Tutti gli algoritmi descritti
in questo file usano questo metodo per effettuare i restart (potrebbe essere parallelizzato in futuro).

Questo codice ha un TODO, poiché dovrebbe verificare che sia il risultato ottimo con ```.is_goal()```
invece di verificare che la fitness del risultato sia minore o uguale al default del costo (ovvero zero).

Ho definito il problema NQueen, (da come si può vedere dall'implementazione [qui](examples/n_queen.rs)) 
in modo tale che è la solzione corretta quando la sua fitness è 0. In questo caso funziona, ma
se voglio un framwork piu' generico dovrei fare questa modifica.

```
Steepest Descend:
        One restart:
          Correctness: 0.1428
          Total Duration: 62.086533ms
          Mean time: 24.834µs
        Number restarts:10
          Correctness: 0.7996
          Total Duration: 369.848106ms
          Mean time: 147.939µs
```

Queste sono le performace di questo algoritmo. Quando viene fatto un tentativo 
ha una frequenza su 2500 test del 0.0032 di trovare un ottimo. Quando inseriamo i restart
vengono risolte piu' frequentemente le istanze del problema (chiaramente).

# Hill Climbing

Esso ha una leggera modifica dell'algoritmo visto a lezione, per cercare di
aumentare la correctness dell'algoritmo ho aggiunto che può fare solo un numero
massimo di mosse laterali e se le supera allora cercherà risultati che abbiano uina fitness
strettamente minore dello stato attuale.

Implementazione:

```rust

    fn get_next_state<P: IterativeImprovingProblem>(
        lateral: &mut usize,
        problem: &P,
        state: &P::State,
        curr_h: P::Cost,
        max_lateral: Option<usize>,
    ) -> Option<(P::State, P::Cost)> {
        let mut actions = problem.executable_actions(state);
        while let Some(a) = actions.next() {
            let (next_state, _) = problem.result(state, &a);
            let next_h = problem.heuristic(&next_state);
            if max_lateral.map_or(true, |x| x > *lateral) && next_h == curr_h {
                *lateral += 1;
                return (next_state, next_h).into();
            }
            if next_h < curr_h {
                *lateral = 0;
                return (next_state, next_h).into();
            }
        }
        None
    }

    fn attempt(&mut self, problem: &P) -> AttemptResult<P> {
        let mut curr_state = problem.random_state(&mut self.rng);
        let mut curr_h = problem.heuristic(&curr_state);
        let mut iterations = 0;
        let mut lateral = 0;
        loop {
            iterations += 1;
            let to_assign =
                Self::get_next_state(&mut lateral, problem, &curr_state, curr_h, self.max_lateral);
            if let Some((next_state, next_h)) = to_assign {
                curr_state = next_state;
                curr_h = next_h;
            } else {
                return AttemptResult::new(curr_state, curr_h, iterations);
            }
        }
    }
```

Il motivo per cui ho una seconda funzione per calcolarmi il prossimo stato
è semplicemnte perché ho litigato con la regola di owership di rust sulle variabili
e volevo evitare di clonare inutilemente lo stato. È da migliorare per la legibilità.

Performance:

```
Hill Climbing:
        One restart:
          Correctness: 0.1908
          Total Duration: 609.368644ms
          Mean time: 243.747µs
        Number restarts:10
          Correctness: 0.862
          Total Duration: 3.370510827s
          Mean time: 1.348204ms
```

Da notare che trova il risultato corretto piu' frequentemente di Steepest Descend, ma ci mette molto
piu' tempo per risolvere il problema, ma almeno riesce a superare le spalle della funzione di fitness.

# Simulated Annealing

Implementazione:

```rust
    fn attempt(&mut self, problem: &P) -> AttemptResult<P> {
        let mut curr_state = problem.random_state(&mut self.rng);
        let mut curr_h = problem.heuristic(&curr_state);

        for t in 0.. {
            let velocity = (self.cooling)(t);
            if velocity <= 0.0 {
                return AttemptResult::new(curr_state, curr_h, t + 1);
            }
            let vicinity: Vec<P::Action> = problem.executable_actions(&curr_state).collect();
            let next_action = vicinity.into_iter().choose(&mut self.rng);
            if let Some(next_action) = next_action {
                let (next_state, _) = problem.result(&curr_state, &next_action);
                let next_h = problem.heuristic(&next_state);
                if next_h <= curr_h {
                    curr_state = next_state;
                    curr_h = next_h;
                } else {
                    let diff: f64 = (curr_h - next_h).abs().into();
                    let r: f64 = self.rng.random();

                    if r <= (1.0 / exp(diff / velocity)) {
                        curr_state = next_state;
                        curr_h = next_h;
                    }
                }
            }
        }

        unreachable!()
    }
```

Esso ha come "punto debole" per l'efficienza che deve calcolare tutti gli stati possibili per
scegliere quello casuale in cui saltare. Dovrei implementare un trait sull'iteratore delle
azioni possibili che permette una scelta casuale direttamente sull'iteratore dove le calcola, permettendomi
cosi di non calcolarmi tutte le azioni possibili.

```
Simulated Annealing:
        One restart:
          Correctness: 0.7736
          Total Duration: 396.527097ms
          Mean time: 158.61µs
        Number restarts:10
          Correctness: 1
          Total Duration: 907.607207ms
          Mean time: 363.042µs
```

Comunque le performance sono molto buone, con una frequenza di risoluzione del problema nel 99% dei casi.

# Conclusione

Purtroppo l'algoritmo esplode molto velocemente, ecco le statistiche per la risulzione del problema con grandezza 100.
Non ho potuto fare delle medie per mancanza di tempo, volevo solo far notare in quanto tempo rispondono gli algoritmi.

Per esempio ecco le performance con solo un tentativo per algoritmo.

```
Steepest Descend:
        One restart:
          Mean time: 4.051691967s
Hill Climbing:
        One restart:
          Mean time: 13.406422981s
Simulated Annealing:
        One restart:
          Total Duration: 18.191596ms
```

Comunque Simulated Anneling non esplode di tanto.